{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde8d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from Clean_Function import  provider_group_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1c2bb",
   "metadata": {},
   "source": [
    "## Load testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87ea6c",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eeb1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Data/df_groupby_provider1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b1496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label Encoding PotentialFraud \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit([\"No\", \"Yes\"])  # 0: Non-fraud (No), 1: fraud (Yes), \n",
    "df_train['PotentialFraud'] = label_encoder.transform(df_train['PotentialFraud']) \n",
    "\n",
    "## Separating x and y for train dataset\n",
    "\n",
    "LR_x = df_train.drop(['PotentialFraud'], axis=1)\n",
    "LR_y = df_train['PotentialFraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57dc492e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    87.302928\n",
       "1    12.697072\n",
       "Name: PotentialFraud, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_y.value_counts(normalize=True)*100 # Imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71632b",
   "metadata": {},
   "source": [
    "#### Split dataset into train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af13284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Shape of X_train data : (5683, 46)\n",
      "Shape of X_test data : (1421, 46)\n",
      "Shape of Y_train data : (5683,)\n",
      "Shape of Y_test data : (1421,)\n",
      "\n",
      "\n",
      "************************************************************\n",
      "\n",
      "\n",
      "Class ratio - Fraud/Non-Fraud (Y_train): \n",
      " 0    87.295443\n",
      "1    12.704557\n",
      "Name: PotentialFraud, dtype: float64\n",
      "Class ratio - Fraud/Non-Fraud (Y_test): \n",
      " 0    87.332864\n",
      "1    12.667136\n",
      "Name: PotentialFraud, dtype: float64\n",
      "\n",
      "\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Split 80:20 \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(LR_x, LR_y, random_state=42,\\\n",
    "                                                shuffle=True, stratify=LR_y, test_size=0.2)\n",
    "\n",
    "# Looking at dataset shapes\n",
    "\n",
    "print('\\n')\n",
    "print('Shape of X_train data :',X_train.shape)\n",
    "print('Shape of X_test data :',X_test.shape)\n",
    "print('Shape of Y_train data :',Y_train.shape)\n",
    "print('Shape of Y_test data :',Y_test.shape)\n",
    "print('\\n')\n",
    "print(\"*\"*60)\n",
    "\n",
    "# Looking at class ratios\n",
    "\n",
    "print('\\n')\n",
    "print('Class ratio - Fraud/Non-Fraud (Y_train): \\n',Y_train.value_counts(normalize=True)*100)\n",
    "print('Class ratio - Fraud/Non-Fraud (Y_test): \\n',Y_test.value_counts(normalize=True)*100)\n",
    "print('\\n')\n",
    "print(\"*\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fadbc27",
   "metadata": {},
   "source": [
    "#### Deal with Imbalanced Data:  \n",
    "- We will only do oversample/undersample in the train set, don't balance the Validation set only. If we balance the Validation set, our model may work well(may get better score in Val) but in the future after deploying, it may not work better.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b961af",
   "metadata": {},
   "source": [
    "Performance Analysis after Resampling\n",
    "To understand the effect of oversampling, I will be using a bank customer churn dataset. It is an imbalanced data where thetarget variable,\n",
    "churn\n",
    "has 81.5% customers not churning and 18.5% customers who have churned.\n",
    "\n",
    "A comparative analysis was done on the dataset using 3 classifi er models: Logistic Regression, Decision Tree, and RandomForest. As discussed earlier, we’ll ignore the accuracy metric to evaluate the performance of the classifi er on this imbalanceddataset. Here, we are more interested to know that which are the customers who’ll churn out in the coming months. Thereby,we’ll focus on metrics like precision, recall, F1-score to understand the performance of the classifi ers for correctly determiningwhich customers will churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f5c1a",
   "metadata": {},
   "source": [
    "1. SMOTE: Synthetic Minority Oversampling Technique\n",
    "- SMOTE is an oversampling technique where the synthetic samples are generated for the minority class. Thisalgorithm helps to overcome the overfi tting problem posed by random oversampling. It focuses on the feature spaceto generate new instances with the help of interpolation between the positive instances that lie together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d406dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Counter({0: 4961, 1: 722})\n",
      "After Counter({0: 4961, 1: 4961})\n"
     ]
    }
   ],
   "source": [
    "counter_before = Counter(Y_train)\n",
    "print('Before', counter_before)\n",
    "X_train_sm, Y_train_sm = SMOTE(random_state=42, k_neighbors=5).fit_resample(X_train, Y_train)\n",
    "counter_after_sm = Counter(Y_train_sm)\n",
    "print('After', counter_after_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6afff",
   "metadata": {},
   "source": [
    "2. Hybridization: SMOTE + ENN\n",
    "- SMOTE + ENN is hybrid technique where more no. of observations are removed from the sample space.Here, ENN is yet another undersampling technique where the nearest neighbors of each of the majority class isestimated. If the nearest neighbors misclassify that particular instance of the majority class, then that instance getsdeleted.\n",
    "Integrating this technique with oversampled data done by SMOTE helps in doing extensive data cleaning. Here onmisclassifi cation by NN’s samples from both the classes are removed. This results in a more clear and concise classseparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fedfe6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Counter({1: 4554, 0: 3199})\n"
     ]
    }
   ],
   "source": [
    "X_train_smenn, Y_train_smenn = SMOTEENN(random_state=42).fit_resample(X_train, Y_train)\n",
    "counter_after_smenn = Counter(Y_train_smenn)\n",
    "print('After', counter_after_smenn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6c555",
   "metadata": {},
   "source": [
    "#### Save the train/validation Original sm smeen dataset for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e09beb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Files\n",
    "X_train.to_csv('Data/X_train.csv',index= False)\n",
    "Y_train.to_csv('Data/Y_train.csv',index= False)\n",
    "X_test.to_csv('Data/X_test.csv',index= False)\n",
    "Y_test.to_csv('Data/Y_test.csv',index= False)\n",
    "\n",
    "# SMOTE files\n",
    "\n",
    "X_train_sm.to_csv('Data/X_train_sm.csv',index= False)\n",
    "Y_train_sm.to_csv('Data/Y_train_sm.csv',index= False)\n",
    "\n",
    "# SMOTE + ENN files\n",
    "\n",
    "X_train_smenn.to_csv('Data/X_train_smenn.csv',index= False)\n",
    "Y_train_smenn.to_csv('Data/Y_train_smenn.csv',index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc8f889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1241, 1: 180})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd91f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
